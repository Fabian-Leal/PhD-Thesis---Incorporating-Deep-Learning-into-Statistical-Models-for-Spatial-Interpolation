{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default='browser'\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy as sp\n",
    "import scipy.cluster\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deposit_data = pd.read_csv(\"../../Curated_data/final_dataset.csv\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_columns = ['CP_Total','PO_Total', 'PY_Total']\n",
    "\n",
    "#all covariates\n",
    "covariates = total_columns[:3] + ['RQD_Pct', 'Cr_ppm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['phi_0', 'phi_1', 'phi_2', 'phi_3', 'phi_4', 'phi_5', 'phi_6', 'phi_7', 'phi_8', 'phi_9']\n"
     ]
    }
   ],
   "source": [
    "# Assuming deposit_data is your DataFrame\n",
    "# Extract the names of the first 98 columns\n",
    "phi_columns = deposit_data.columns[10:].tolist()\n",
    "\n",
    "# Display the list of column names\n",
    "print(phi_columns[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "deposit_data = deposit_data.dropna(subset=['Density_gcm3'] + covariates + phi_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = len(phi_columns) + len(covariates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepkriging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Fold 1:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0031\n",
      "  RMSE: 0.0561\n",
      "  MAE: 0.0405\n",
      "  R^2: 0.8029\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [440], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m601\u001b[39m):\n\u001b[1;32m---> 74\u001b[0m     pre \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     mse \u001b[38;5;241m=\u001b[39m mse_loss(pre, torch\u001b[38;5;241m.\u001b[39mtensor(y_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     76\u001b[0m     cost \u001b[38;5;241m=\u001b[39m mse\n",
      "File \u001b[1;32mc:\\Users\\23478671\\Anaconda3\\envs\\geostatistics\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\23478671\\Anaconda3\\envs\\geostatistics\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\23478671\\Anaconda3\\envs\\geostatistics\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\23478671\\Anaconda3\\envs\\geostatistics\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to print evaluation metrics\n",
    "def print_metrics(actual, predicted, set_name):\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "\n",
    "    print(f\"Metrics for {set_name} set:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R^2: {r2:.4f}\\n\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "phi_columns = deposit_data.columns[10:].tolist()\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Assuming deposit_data, covariates, and other necessary variables are defined\n",
    "\n",
    "# Create an array to store metrics for each fold\n",
    "test_mse_list = []\n",
    "test_rmse_list = []\n",
    "test_mae_list = []\n",
    "test_r2_list = []\n",
    "\n",
    "\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 10\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(deposit_data)):\n",
    "    train_data, test_data = deposit_data.iloc[train_index], deposit_data.iloc[test_index]\n",
    "\n",
    "    x_train = train_data[phi_columns + covariates].values\n",
    "    y_train = train_data['Density_gcm3'].values\n",
    "\n",
    "    x_test = test_data[phi_columns + covariates].values\n",
    "    y_test = test_data['Density_gcm3'].values\n",
    "\n",
    "    # Define your neural network\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(in_features=p, out_features=100),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5) ,\n",
    "        nn.BatchNorm1d(100),\n",
    "        nn.Linear(in_features=100, out_features=100),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(in_features=100, out_features=100),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(100),\n",
    "        nn.Linear(in_features=100, out_features=1))\n",
    "\n",
    "\n",
    "    mse_loss = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_losses = []  # To store training losses during training\n",
    "    test_losses = []   # To store test losses during training\n",
    "\n",
    "    # Training loop\n",
    "    for step in range(601):\n",
    "        pre = model(torch.tensor(x_train, dtype=torch.float32))\n",
    "        mse = mse_loss(pre, torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32))\n",
    "        cost = mse\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pre_test = model(torch.tensor(x_test, dtype=torch.float32))\n",
    "        mse_test = mse_loss(pre_test, torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32))\n",
    "        test_losses.append(mse_test.item())\n",
    "\n",
    "    # Store metrics for this fold\n",
    "    test_predictions_fold = model(torch.tensor(x_test, dtype=torch.float32)).detach().numpy().flatten()\n",
    "    test_mse_list.append(mean_squared_error(y_test, test_predictions_fold))\n",
    "    test_mae_list.append(mean_absolute_error(y_test, test_predictions_fold))\n",
    "    test_r2_list.append(r2_score(y_test, test_predictions_fold))\n",
    "\n",
    "\n",
    "\n",
    "    # Print metrics for the current fold\n",
    "    print(f\"\\nMetrics for Fold {fold + 1}:\")\n",
    "    print_metrics(y_test, test_predictions_fold, \"Test\")\n",
    "\n",
    "# Print average metrics across folds\n",
    "print(\"\\nAverage Metrics Across Folds:\")\n",
    "print(f\"  Average MSE: {np.mean(test_mse_list):.4f}\")\n",
    "print(f\"  Average MAE: {np.mean(test_mae_list):.4f}\")\n",
    "print(f\"  Average R2: {np.mean(test_r2_list):.4f}\")\n",
    "print(f\"  STD MSE: {np.std(test_mse_list):.4f}\")\n",
    "print(f\"  STD MAE: {np.std(test_mae_list):.4f}\")\n",
    "print(f\"  STD R2: {np.std(test_r2_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Modified Deepkriging (separate phi branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Fold 1:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0039\n",
      "  RMSE: 0.0626\n",
      "  MAE: 0.0430\n",
      "  R^2: 0.7548\n",
      "\n",
      "\n",
      "Metrics for Fold 2:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0033\n",
      "  RMSE: 0.0571\n",
      "  MAE: 0.0374\n",
      "  R^2: 0.7512\n",
      "\n",
      "\n",
      "Metrics for Fold 3:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0039\n",
      "  RMSE: 0.0627\n",
      "  MAE: 0.0421\n",
      "  R^2: 0.7330\n",
      "\n",
      "\n",
      "Metrics for Fold 4:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0038\n",
      "  RMSE: 0.0615\n",
      "  MAE: 0.0399\n",
      "  R^2: 0.7196\n",
      "\n",
      "\n",
      "Metrics for Fold 5:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0035\n",
      "  RMSE: 0.0595\n",
      "  MAE: 0.0428\n",
      "  R^2: 0.8034\n",
      "\n",
      "\n",
      "Metrics for Fold 6:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0029\n",
      "  RMSE: 0.0535\n",
      "  MAE: 0.0379\n",
      "  R^2: 0.7752\n",
      "\n",
      "\n",
      "Metrics for Fold 7:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0027\n",
      "  RMSE: 0.0519\n",
      "  MAE: 0.0360\n",
      "  R^2: 0.8507\n",
      "\n",
      "\n",
      "Metrics for Fold 8:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0035\n",
      "  RMSE: 0.0590\n",
      "  MAE: 0.0421\n",
      "  R^2: 0.8066\n",
      "\n",
      "\n",
      "Metrics for Fold 9:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0045\n",
      "  RMSE: 0.0673\n",
      "  MAE: 0.0458\n",
      "  R^2: 0.6428\n",
      "\n",
      "\n",
      "Metrics for Fold 10:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0047\n",
      "  RMSE: 0.0682\n",
      "  MAE: 0.0460\n",
      "  R^2: 0.7636\n",
      "\n",
      "\n",
      "Average Metrics Across Folds:\n",
      "  Average MSE: 0.0037\n",
      "  Average MAE: 0.0413\n",
      "  Average R2: 0.7601\n",
      "  STD MSE: 0.0006\n",
      "  STD MAE: 0.0033\n",
      "  STD R2: 0.0536\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Function to print evaluation metrics\n",
    "def print_metrics(actual, predicted, set_name):\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "\n",
    "    print(f\"Metrics for {set_name} set:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R^2: {r2:.4f}\\n\")\n",
    "\n",
    "# Assuming deposit_data, covariates, and other necessary variables are defined\n",
    "\n",
    "# Create an array to store metrics for each fold\n",
    "test_mse_list = []\n",
    "test_mae_list = []\n",
    "test_r2_list = []\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 10\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Define a neural network with separate branches for phi_columns and covariates\n",
    "class GroupedNet(nn.Module):\n",
    "    def __init__(self, input_size_phi, input_size_covariates, output_size):\n",
    "        super(GroupedNet, self).__init__()\n",
    "        self.phi_branch = nn.Sequential(\n",
    "            nn.Linear(in_features=input_size_phi, out_features=1)\n",
    "     \n",
    "        )\n",
    "\n",
    "        self.combine_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=input_size_covariates+1, out_features=100), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Linear(in_features=100, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=100, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Linear(in_features=100, out_features=1)\n",
    "        )\n",
    "                                           \n",
    "\n",
    "    def forward(self, input_phi, input_covariates):\n",
    "        phi_output = self.phi_branch(input_phi)\n",
    "        x = torch.cat((phi_output, input_covariates), dim=1)\n",
    "        output = self.combine_layer(x)\n",
    "        return output\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(deposit_data)):\n",
    "    train_data, test_data = deposit_data.iloc[train_index], deposit_data.iloc[test_index]\n",
    "\n",
    "    x_train_phi = train_data[phi_columns].values\n",
    "    x_train_covariates = train_data[covariates].values\n",
    "    y_train = train_data['Density_gcm3'].values\n",
    "\n",
    "    x_test_phi = test_data[phi_columns].values\n",
    "    x_test_covariates = test_data[covariates].values\n",
    "    y_test = test_data['Density_gcm3'].values\n",
    "\n",
    "    # Define your neural network\n",
    "    model = GroupedNet(input_size_phi=len(phi_columns), input_size_covariates=len(covariates), output_size=1)\n",
    "\n",
    "    mse_loss = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_losses = []  # To store training losses during training\n",
    "    test_losses = []   # To store test losses during training\n",
    "\n",
    "    # Training loop\n",
    "    for step in range(601):\n",
    "        pre = model(torch.tensor(x_train_phi, dtype=torch.float32), torch.tensor(x_train_covariates, dtype=torch.float32))\n",
    "        mse = mse_loss(pre, torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32))\n",
    "        cost = mse\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pre_test = model(torch.tensor(x_test_phi, dtype=torch.float32), torch.tensor(x_test_covariates, dtype=torch.float32))\n",
    "        mse_test = mse_loss(pre_test, torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32))\n",
    "        test_losses.append(mse_test.item())\n",
    "\n",
    "    # Store metrics for this fold\n",
    "    test_predictions_fold = model(torch.tensor(x_test_phi, dtype=torch.float32), torch.tensor(x_test_covariates, dtype=torch.float32)).detach().numpy().flatten()\n",
    "    test_mse_list.append(mean_squared_error(y_test, test_predictions_fold))\n",
    "    test_mae_list.append(mean_absolute_error(y_test, test_predictions_fold))\n",
    "    test_r2_list.append(r2_score(y_test, test_predictions_fold))\n",
    "\n",
    "    # Print metrics for the current fold\n",
    "    print(f\"\\nMetrics for Fold {fold + 1}:\")\n",
    "    print_metrics(y_test, test_predictions_fold, \"Test\")\n",
    "\n",
    "# Print average\n",
    "# Print average metrics across folds\n",
    "print(\"\\nAverage Metrics Across Folds:\")\n",
    "print(f\"  Average MSE: {np.mean(test_mse_list):.4f}\")\n",
    "print(f\"  Average MAE: {np.mean(test_mae_list):.4f}\")\n",
    "print(f\"  Average R2: {np.mean(test_r2_list):.4f}\")\n",
    "print(f\"  STD MSE: {np.std(test_mse_list):.4f}\")\n",
    "print(f\"  STD MAE: {np.std(test_mae_list):.4f}\")\n",
    "print(f\"  STD R2: {np.std(test_r2_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('phi_branch.0.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('phi_branch.0.bias', tensor([0.0506])),\n",
       "             ('combine_layer.0.weight',\n",
       "              tensor([[ 5.2305e-01,  2.0649e-02,  1.4947e-01, -2.8930e-01,  2.3886e-01,\n",
       "                        3.6180e-01],\n",
       "                      [-5.1055e-01,  1.5531e-01, -2.4170e-02, -1.4622e-01, -1.0970e-02,\n",
       "                       -2.0758e-02],\n",
       "                      [-3.1595e-01,  2.8055e-03,  3.5709e-01, -3.9006e-01,  3.0231e-01,\n",
       "                        2.5293e-02],\n",
       "                      [ 4.7832e-01, -1.8272e-01,  6.1926e-02, -5.0744e-02,  5.0942e-02,\n",
       "                        1.8982e-01],\n",
       "                      [-3.0387e-02, -3.0791e-03, -1.2362e-01, -3.4957e-03,  3.1578e-01,\n",
       "                       -2.8892e-01],\n",
       "                      [-4.1537e-01,  2.7560e-01, -2.4751e-01, -1.0008e-01, -1.7827e-01,\n",
       "                       -2.8337e-01],\n",
       "                      [-3.6060e-01,  2.8669e-01, -2.1583e-01,  1.9986e-01,  1.6637e-01,\n",
       "                       -2.5617e-02],\n",
       "                      [ 3.7846e-01, -2.6673e-01,  3.4062e-01, -5.6116e-02, -1.9757e-01,\n",
       "                        9.5801e-02],\n",
       "                      [ 2.0607e-01,  3.0934e-01,  8.9168e-02, -2.2434e-02, -3.7923e-01,\n",
       "                       -3.1157e-01],\n",
       "                      [-4.7114e-01, -6.9286e-02, -1.5061e-01, -3.6295e-01, -1.2570e-01,\n",
       "                       -3.2957e-01],\n",
       "                      [-6.3329e-01,  3.7839e-01,  1.1767e-01,  2.1809e-01,  2.5093e-01,\n",
       "                        1.3485e-01],\n",
       "                      [ 6.1951e-01, -2.0835e-01, -2.5996e-01,  3.6999e-01,  1.8095e-01,\n",
       "                       -1.6178e-01],\n",
       "                      [ 3.6248e-01,  9.1959e-02,  6.8103e-02,  3.1700e-02,  3.6366e-01,\n",
       "                        2.8710e-01],\n",
       "                      [ 3.4812e-01, -2.0135e-01, -2.1525e-01, -9.0587e-02, -7.0770e-02,\n",
       "                       -2.0396e-01],\n",
       "                      [ 9.3391e-02,  1.7273e-01,  2.2127e-01,  4.0342e-01, -2.9362e-01,\n",
       "                       -3.6173e-01],\n",
       "                      [-1.0192e-01, -2.4457e-01,  2.8073e-01,  3.3916e-01,  1.0432e-01,\n",
       "                        2.1506e-01],\n",
       "                      [ 4.2456e-01,  3.2246e-01,  3.8754e-01, -8.2936e-02, -3.2488e-01,\n",
       "                        1.4567e-01],\n",
       "                      [ 5.0656e-01,  8.3039e-03, -4.7611e-02, -1.6751e-01,  1.0658e-01,\n",
       "                       -1.0487e-01],\n",
       "                      [ 3.9810e-01, -3.0091e-02,  2.3555e-01,  3.3847e-01, -5.8608e-02,\n",
       "                       -2.4246e-01],\n",
       "                      [-5.3061e-01,  7.1463e-02,  4.6097e-02,  2.9177e-01, -3.3941e-01,\n",
       "                        2.6095e-02],\n",
       "                      [-3.1315e-01, -2.4882e-02, -1.1545e-01,  1.3616e-01,  6.3969e-02,\n",
       "                       -1.4105e-01],\n",
       "                      [-3.0633e-01, -1.9250e-02, -2.1516e-01, -4.1089e-01,  1.4151e-01,\n",
       "                        9.6697e-02],\n",
       "                      [ 1.0653e-02, -4.0838e-01, -1.7128e-01, -1.0485e-01, -3.4429e-01,\n",
       "                       -3.4384e-01],\n",
       "                      [-1.5983e-01,  2.9760e-01, -3.2725e-01,  3.4731e-01, -1.4223e-01,\n",
       "                       -6.7215e-02],\n",
       "                      [ 1.3632e-01, -2.7389e-01,  3.8526e-01, -1.0275e-01,  1.5325e-01,\n",
       "                       -2.1320e-01],\n",
       "                      [-5.3356e-01, -2.4774e-01, -2.9887e-02, -1.9866e-01, -3.4363e-02,\n",
       "                        2.4481e-01],\n",
       "                      [-3.0454e-01, -1.6007e-03,  7.8273e-02, -2.1942e-01, -1.1131e-01,\n",
       "                       -2.3903e-01],\n",
       "                      [-2.3842e-01,  3.6508e-01, -2.9954e-01,  3.0787e-01, -9.1489e-02,\n",
       "                        3.1794e-01],\n",
       "                      [-6.0924e-01, -2.0729e-01, -8.8922e-02, -2.4451e-01, -1.6416e-01,\n",
       "                       -1.1329e-01],\n",
       "                      [-2.5726e-01, -7.8114e-02, -3.6851e-03, -3.9739e-01,  1.7596e-01,\n",
       "                        1.1121e-01],\n",
       "                      [ 1.5980e-02, -3.9280e-01,  2.9079e-01, -1.7393e-01,  3.4007e-01,\n",
       "                        2.3273e-01],\n",
       "                      [-3.9527e-01, -1.8119e-01, -2.3802e-01, -8.4745e-02, -3.2174e-01,\n",
       "                       -2.5542e-01],\n",
       "                      [-6.3742e-02, -2.6274e-01, -3.0800e-01, -8.6272e-02, -3.3618e-01,\n",
       "                       -3.7484e-01],\n",
       "                      [ 4.4897e-03,  3.8862e-01,  2.1502e-01, -7.4184e-02,  3.2983e-02,\n",
       "                       -1.5798e-02],\n",
       "                      [ 6.2131e-03,  2.3875e-01, -1.6447e-01, -4.1257e-02,  3.0737e-01,\n",
       "                       -3.6236e-01],\n",
       "                      [ 3.6316e-01,  2.1235e-02, -4.0425e-01,  1.1916e-01, -3.0804e-01,\n",
       "                       -4.0287e-01],\n",
       "                      [-3.5109e-01, -2.5611e-01, -9.7117e-02, -6.4142e-02, -7.6059e-02,\n",
       "                       -1.7961e-01],\n",
       "                      [-4.0438e-01, -3.7110e-02, -4.4574e-02, -2.8040e-01,  1.8053e-01,\n",
       "                        2.7072e-01],\n",
       "                      [-5.4509e-01,  8.0330e-02, -2.5325e-01, -2.5652e-01,  1.1136e-01,\n",
       "                       -3.8576e-01],\n",
       "                      [ 4.4063e-01,  1.0966e-01,  9.7823e-02, -8.1470e-02,  1.2374e-01,\n",
       "                       -4.0454e-01],\n",
       "                      [-8.6784e-03,  1.1315e-02,  3.1894e-02,  2.1314e-01,  8.8395e-02,\n",
       "                        2.3169e-01],\n",
       "                      [ 2.1234e-01, -2.5522e-02, -3.4802e-01, -2.8098e-01, -1.6952e-01,\n",
       "                       -2.2540e-01],\n",
       "                      [-5.8619e-01,  3.7471e-01,  1.9039e-01,  1.9244e-01,  2.3012e-01,\n",
       "                        4.7264e-02],\n",
       "                      [-6.6717e-01,  9.2855e-02,  1.6566e-01,  8.7788e-02,  3.2911e-01,\n",
       "                        2.4430e-01],\n",
       "                      [ 4.4587e-01, -8.2603e-02,  1.6306e-01, -2.7708e-01,  5.8937e-02,\n",
       "                        3.7701e-01],\n",
       "                      [-3.7116e-02,  1.6784e-01,  3.2767e-01,  2.5938e-01,  2.8261e-01,\n",
       "                       -3.0714e-02],\n",
       "                      [-1.1542e-01,  3.1620e-01, -2.0079e-01, -2.8077e-01,  1.1188e-01,\n",
       "                        9.4902e-02],\n",
       "                      [-7.0161e-02, -2.5259e-01,  1.1022e-01, -2.1306e-01,  6.7747e-02,\n",
       "                        1.3412e-01],\n",
       "                      [-5.3387e-01,  1.6867e-01,  1.8788e-01,  1.0466e-01,  2.3932e-01,\n",
       "                        9.6348e-02],\n",
       "                      [ 1.3928e-01, -8.2849e-03,  2.0715e-01,  2.5419e-01, -1.8639e-01,\n",
       "                        3.9887e-01],\n",
       "                      [ 3.6971e-01, -2.6638e-01, -2.8380e-01, -3.0691e-01,  1.1306e-01,\n",
       "                        1.0331e-01],\n",
       "                      [ 1.2630e-01,  1.9767e-02,  1.0083e-01,  2.9947e-01,  3.7524e-01,\n",
       "                       -6.1873e-02],\n",
       "                      [-1.4675e-01,  1.2728e-01,  2.6699e-01,  2.7189e-01, -1.9574e-01,\n",
       "                       -2.2320e-01],\n",
       "                      [-3.6417e-01,  1.7445e-01, -2.6651e-01,  2.1811e-01, -2.9768e-01,\n",
       "                       -1.2380e-02],\n",
       "                      [-7.1897e-02,  5.7004e-02, -1.3906e-01,  2.4310e-01, -2.5878e-01,\n",
       "                       -1.8565e-01],\n",
       "                      [ 7.0465e-02,  1.4233e-01,  3.1025e-01,  1.3915e-01, -2.2484e-01,\n",
       "                       -1.0719e-01],\n",
       "                      [-1.6393e-01, -2.3541e-02,  2.5389e-01, -2.3160e-01,  2.3783e-01,\n",
       "                        7.2253e-03],\n",
       "                      [ 4.3659e-01,  8.4573e-02,  3.1324e-01, -2.9912e-01, -3.5984e-01,\n",
       "                       -3.1341e-02],\n",
       "                      [ 4.9086e-01,  1.3162e-01, -2.5177e-01,  5.3737e-03,  1.8658e-01,\n",
       "                        2.3504e-01],\n",
       "                      [ 4.0226e-01, -1.3739e-01, -2.2238e-01,  1.4467e-01, -1.2721e-01,\n",
       "                        2.4311e-01],\n",
       "                      [ 1.0421e-01,  3.2877e-01, -9.7885e-02, -1.3941e-02,  2.2338e-01,\n",
       "                        1.2347e-01],\n",
       "                      [-2.3819e-01, -1.9067e-01, -1.2806e-01, -8.5517e-02, -7.4671e-03,\n",
       "                        3.6252e-01],\n",
       "                      [-4.0925e-01, -1.3474e-01,  4.7890e-02,  2.1113e-03, -1.3109e-01,\n",
       "                       -9.5651e-02],\n",
       "                      [ 3.5921e-01, -1.0017e-01, -1.3239e-01, -8.3913e-02,  3.5044e-01,\n",
       "                       -1.8539e-01],\n",
       "                      [ 5.0302e-01, -5.8519e-02, -1.4871e-01, -8.4726e-02, -1.6496e-01,\n",
       "                        8.4807e-02],\n",
       "                      [-3.7335e-01,  2.3280e-01, -1.3790e-01,  3.2529e-01,  3.9031e-01,\n",
       "                        2.5185e-01],\n",
       "                      [ 4.5540e-01, -1.7217e-02,  2.3403e-01, -4.3375e-03, -2.5741e-01,\n",
       "                        1.7817e-01],\n",
       "                      [ 3.2177e-01, -1.2182e-01, -7.3512e-03, -5.8891e-02,  1.3935e-01,\n",
       "                       -8.6125e-02],\n",
       "                      [-3.9826e-01,  2.2131e-01, -3.2500e-01, -2.3958e-01, -5.9591e-02,\n",
       "                        1.2798e-01],\n",
       "                      [ 1.7597e-01, -1.9209e-01,  2.7917e-01, -4.3843e-02,  1.5974e-01,\n",
       "                        8.0090e-02],\n",
       "                      [ 2.8315e-01, -3.9167e-01, -3.1603e-01,  1.0565e-01,  3.9137e-02,\n",
       "                       -1.3554e-02],\n",
       "                      [ 1.8683e-01,  1.2213e-01, -1.8762e-01,  2.5754e-03, -2.6896e-01,\n",
       "                        6.6796e-02],\n",
       "                      [ 1.9432e-01,  5.3073e-02,  3.5211e-02,  2.4198e-01, -1.2054e-01,\n",
       "                       -6.0879e-02],\n",
       "                      [-5.9684e-01,  3.6630e-01,  6.0045e-02,  3.7719e-01,  2.3520e-01,\n",
       "                       -1.0378e-02],\n",
       "                      [-3.1417e-02,  8.1131e-02, -3.5719e-01, -2.5249e-01, -1.4259e-02,\n",
       "                        2.0066e-02],\n",
       "                      [ 4.1792e-01, -3.8451e-03, -1.6812e-01,  2.8232e-01, -1.6900e-01,\n",
       "                        3.6230e-01],\n",
       "                      [ 8.9893e-02, -2.1583e-01,  1.2350e-01, -1.9259e-01, -1.3262e-01,\n",
       "                       -2.7901e-01],\n",
       "                      [ 2.7659e-01,  1.7668e-01,  8.1581e-02, -1.6750e-01, -1.9421e-01,\n",
       "                       -1.4568e-01],\n",
       "                      [ 5.7218e-01,  6.0427e-02, -3.7782e-01, -2.6952e-01,  1.3716e-01,\n",
       "                        3.3358e-01],\n",
       "                      [-4.9511e-01,  1.2269e-01,  1.7035e-02, -2.4315e-01,  1.6066e-01,\n",
       "                       -2.6823e-01],\n",
       "                      [-1.0671e-01, -7.9644e-02,  3.1954e-01, -7.8574e-02, -1.6431e-01,\n",
       "                       -1.6662e-01],\n",
       "                      [ 7.4965e-02, -1.0496e-01, -1.9440e-01,  2.5720e-01, -3.5295e-01,\n",
       "                        1.1492e-01],\n",
       "                      [ 3.2608e-01,  3.0785e-01, -2.9921e-01,  4.0989e-02, -1.9528e-01,\n",
       "                        5.8099e-02],\n",
       "                      [ 3.9048e-01, -6.7434e-02, -8.5468e-02, -3.8947e-01, -3.6103e-01,\n",
       "                       -3.5267e-01],\n",
       "                      [ 2.5009e-01,  7.6592e-02,  3.7959e-01, -2.8231e-01,  2.1584e-01,\n",
       "                       -2.2336e-01],\n",
       "                      [-3.9973e-01, -1.4138e-01,  2.9265e-01,  1.5909e-01, -1.6705e-01,\n",
       "                       -1.6147e-01],\n",
       "                      [-6.5958e-04, -3.9479e-01,  2.3199e-01, -3.6760e-01, -4.0081e-01,\n",
       "                        6.3693e-02],\n",
       "                      [-4.2616e-01,  8.4508e-03, -1.1871e-01,  1.0618e-01, -1.6376e-01,\n",
       "                       -2.6343e-01],\n",
       "                      [-1.8633e-01,  8.6501e-02,  1.2569e-01, -3.9651e-01,  1.7262e-01,\n",
       "                       -9.0804e-02],\n",
       "                      [-3.8545e-01, -2.2933e-01,  2.3952e-02, -3.7184e-02, -2.7673e-01,\n",
       "                       -3.8213e-01],\n",
       "                      [ 1.8004e-01, -1.0708e-01,  1.3804e-01, -1.1769e-02, -2.6667e-01,\n",
       "                        3.6072e-01],\n",
       "                      [-2.8881e-01,  1.9689e-02, -2.8414e-01, -3.1654e-01,  1.9600e-01,\n",
       "                        1.6213e-01],\n",
       "                      [ 9.7872e-02, -2.7961e-01,  2.4018e-02,  1.9991e-01, -3.7658e-01,\n",
       "                       -3.1899e-01],\n",
       "                      [-3.7244e-01, -3.8566e-01,  1.1241e-01, -1.4083e-01, -2.7102e-01,\n",
       "                       -1.3369e-02],\n",
       "                      [-3.7272e-01, -2.4233e-01,  2.9680e-01,  2.1545e-01,  6.8143e-04,\n",
       "                       -4.1786e-01],\n",
       "                      [ 1.4273e-01, -1.0458e-01,  1.2385e-01,  1.5788e-01, -2.9865e-02,\n",
       "                       -6.0695e-02],\n",
       "                      [-4.7932e-02, -3.1680e-01,  3.3937e-01,  3.4961e-01, -1.2595e-01,\n",
       "                        2.2919e-01],\n",
       "                      [-4.4004e-02,  2.3802e-01,  4.0019e-01,  2.3087e-01,  2.8173e-02,\n",
       "                        1.0684e-01],\n",
       "                      [-6.4365e-01, -2.2395e-01,  2.0021e-01,  3.2846e-01,  2.6492e-01,\n",
       "                        2.6642e-01],\n",
       "                      [ 1.6485e-01,  1.4918e-01, -1.7941e-01,  3.1046e-01, -1.9433e-01,\n",
       "                       -3.3949e-01]])),\n",
       "             ('combine_layer.0.bias',\n",
       "              tensor([ 0.0917,  0.0597, -0.3185,  0.0766,  0.1072,  0.1585, -0.1757,  0.1287,\n",
       "                      -0.3989,  0.1680, -0.0890,  0.2801,  0.2505, -0.1902, -0.1250,  0.2029,\n",
       "                       0.0456,  0.0211,  0.1929,  0.2903,  0.0102,  0.0164,  0.0406, -0.0208,\n",
       "                       0.1854, -0.0147,  0.1561, -0.3011,  0.2863, -0.2010,  0.0793,  0.3496,\n",
       "                       0.2074, -0.2834,  0.2557, -0.1663,  0.1693, -0.2402,  0.1441,  0.0908,\n",
       "                       0.2339, -0.3270,  0.0134, -0.2221,  0.3206,  0.3086, -0.2372, -0.3387,\n",
       "                      -0.3310, -0.2785, -0.0562, -0.0154,  0.0106,  0.2843, -0.2975, -0.0911,\n",
       "                      -0.2383,  0.1957, -0.1355, -0.3697,  0.2751, -0.2443,  0.1407, -0.1538,\n",
       "                       0.1167,  0.1476,  0.1375, -0.0754, -0.0155,  0.3864, -0.2247,  0.2408,\n",
       "                      -0.2411,  0.3170, -0.0838,  0.0724, -0.0262,  0.0772,  0.0340,  0.0242,\n",
       "                      -0.1530, -0.1691, -0.2353,  0.3556, -0.3028,  0.1833, -0.2244,  0.1785,\n",
       "                       0.3082, -0.3832, -0.3932, -0.2236,  0.1816, -0.0902,  0.1311, -0.0206,\n",
       "                       0.0391,  0.2669, -0.0798,  0.1943])),\n",
       "             ('combine_layer.3.weight',\n",
       "              tensor([0.9449, 1.0069, 1.0246, 0.9502, 0.9576, 0.9950, 0.9991, 1.0265, 1.0000,\n",
       "                      1.0250, 0.9660, 0.9722, 0.9345, 1.0000, 0.9873, 0.9421, 1.0009, 0.9816,\n",
       "                      0.9894, 1.0412, 0.9880, 0.9270, 1.0067, 0.9966, 0.9432, 1.0467, 1.0043,\n",
       "                      1.0007, 1.0167, 1.0359, 0.9186, 1.0174, 0.9932, 0.9866, 0.9618, 1.0000,\n",
       "                      1.0198, 0.9925, 0.9709, 0.9741, 0.9536, 1.0000, 0.9582, 0.9780, 0.9303,\n",
       "                      0.9499, 1.0018, 1.0000, 1.0280, 1.0046, 1.0058, 0.9731, 1.0151, 0.9986,\n",
       "                      1.0000, 0.9912, 1.0224, 1.0463, 0.9898, 1.0000, 0.9530, 1.0085, 1.0768,\n",
       "                      1.0052, 1.0090, 0.9112, 1.0600, 0.9885, 1.0173, 0.9539, 0.9572, 1.0380,\n",
       "                      0.9966, 0.9850, 1.0000, 1.0064, 1.0000, 0.9932, 0.9703, 0.9811, 1.0000,\n",
       "                      0.9903, 0.9884, 1.0493, 0.9905, 1.0323, 1.0000, 1.0440, 0.9413, 1.0000,\n",
       "                      1.0000, 1.0005, 1.0231, 1.0000, 1.0027, 0.9883, 1.0284, 0.9556, 0.9663,\n",
       "                      0.9896])),\n",
       "             ('combine_layer.3.bias',\n",
       "              tensor([ 0.0112, -0.0278, -0.0073, -0.0088,  0.0144, -0.0288,  0.0437,  0.0243,\n",
       "                       0.0874,  0.0448, -0.0023,  0.0249,  0.0039,  0.0842, -0.0369,  0.0052,\n",
       "                      -0.0115, -0.0168,  0.0314,  0.0053,  0.0392, -0.0030, -0.0041,  0.0045,\n",
       "                       0.0154,  0.0608, -0.0054,  0.0317,  0.0241,  0.0044, -0.0641, -0.0057,\n",
       "                       0.0104,  0.0236,  0.0343,  0.1130,  0.0056,  0.0185,  0.0176,  0.0479,\n",
       "                      -0.0476,  0.1027,  0.0013, -0.0249,  0.0327,  0.0427,  0.0051,  0.1745,\n",
       "                      -0.0058,  0.0399, -0.0011, -0.0151,  0.0638,  0.0389,  0.1023,  0.0180,\n",
       "                      -0.0089,  0.0148, -0.0433,  0.1466,  0.0064,  0.0075,  0.0728, -0.0654,\n",
       "                       0.0035,  0.0628,  0.0301, -0.0478,  0.0267,  0.0284,  0.0888,  0.0127,\n",
       "                       0.1373,  0.0509, -0.1233,  0.0116, -0.1424,  0.0584, -0.0711, -0.0441,\n",
       "                      -0.0753,  0.0624, -0.0230,  0.0603,  0.0172, -0.0126, -0.1399,  0.0201,\n",
       "                      -0.0206, -0.1519,  0.1496, -0.0307,  0.0574,  0.1333,  0.0250, -0.0277,\n",
       "                       0.0458, -0.0236, -0.0149, -0.0061])),\n",
       "             ('combine_layer.3.running_mean',\n",
       "              tensor([4.4761e-01, 4.6384e-02, 1.6352e-02, 1.9239e-01, 2.9346e-01, 1.4940e-02,\n",
       "                      2.2843e-02, 4.1998e-02, 0.0000e+00, 1.7473e-02, 1.9315e-01, 3.9639e-01,\n",
       "                      6.9941e-01, 0.0000e+00, 1.0038e-04, 3.2920e-01, 3.6493e-02, 1.4145e-01,\n",
       "                      1.5330e-01, 4.6022e-02, 2.2806e-02, 1.0691e-01, 7.9798e-06, 1.2434e-03,\n",
       "                      2.6516e-01, 3.0936e-02, 1.8996e-02, 5.6113e-04, 6.0993e-02, 1.0690e-02,\n",
       "                      3.9717e-01, 2.2206e-02, 1.9471e-03, 4.2514e-03, 4.5141e-01, 0.0000e+00,\n",
       "                      2.9393e-02, 2.2452e-02, 9.2456e-02, 1.7967e-01, 3.7764e-01, 0.0000e+00,\n",
       "                      2.6689e-01, 1.2031e-01, 4.9703e-01, 5.9878e-01, 3.6206e-03, 0.0000e+00,\n",
       "                      2.0347e-02, 2.5808e-04, 4.9599e-02, 3.2940e-01, 8.5791e-04, 4.0059e-02,\n",
       "                      0.0000e+00, 4.1352e-04, 1.1136e-02, 3.0948e-02, 1.4683e-01, 0.0000e+00,\n",
       "                      5.6258e-01, 3.5115e-03, 2.3661e-02, 1.2380e-01, 4.8446e-02, 5.5033e-01,\n",
       "                      5.9001e-02, 5.0244e-02, 1.7211e-02, 5.5368e-01, 1.1294e-19, 5.6543e-02,\n",
       "                      1.6956e-43, 5.4213e-01, 0.0000e+00, 7.1227e-02, 0.0000e+00, 1.1457e-02,\n",
       "                      2.6409e-01, 8.6761e-02, 5.6052e-45, 7.6532e-06, 1.6284e-05, 2.6863e-02,\n",
       "                      8.7285e-03, 2.9843e-02, 0.0000e+00, 1.8112e-02, 4.3347e-01, 0.0000e+00,\n",
       "                      0.0000e+00, 1.3233e-02, 3.1053e-03, 0.0000e+00, 3.9399e-02, 2.3476e-03,\n",
       "                      2.4093e-02, 4.0319e-01, 1.5327e-01, 1.8976e-02])),\n",
       "             ('combine_layer.3.running_var',\n",
       "              tensor([2.4394e-01, 1.5702e-02, 3.6539e-03, 5.4623e-02, 1.0105e-01, 3.7587e-03,\n",
       "                      5.3595e-03, 9.4995e-03, 5.6052e-45, 5.3025e-03, 8.0975e-02, 2.0056e-01,\n",
       "                      5.1752e-01, 5.6052e-45, 8.4837e-06, 1.2272e-01, 1.0605e-02, 3.3736e-02,\n",
       "                      3.9801e-02, 2.0165e-02, 5.1715e-03, 2.3494e-02, 3.2434e-07, 2.3511e-04,\n",
       "                      8.4145e-02, 1.3378e-02, 4.2436e-03, 1.0897e-04, 2.6262e-02, 1.9410e-03,\n",
       "                      1.7530e-01, 6.8434e-03, 3.3661e-04, 9.6450e-04, 2.3597e-01, 5.6052e-45,\n",
       "                      7.1813e-03, 7.7580e-03, 3.3273e-02, 5.7162e-02, 1.4830e-01, 5.6052e-45,\n",
       "                      1.1184e-01, 5.5021e-02, 2.8201e-01, 3.9041e-01, 6.5729e-04, 5.6052e-45,\n",
       "                      7.0153e-03, 2.6415e-05, 8.8363e-03, 1.2504e-01, 1.7610e-04, 1.3627e-02,\n",
       "                      5.6052e-45, 6.8425e-05, 1.4774e-03, 8.7637e-03, 3.8718e-02, 5.6052e-45,\n",
       "                      3.2842e-01, 8.1324e-04, 6.2079e-03, 2.7104e-02, 9.1945e-03, 3.3797e-01,\n",
       "                      1.3603e-02, 6.3055e-03, 4.3364e-03, 3.1684e-01, 2.8926e-21, 1.1797e-02,\n",
       "                      5.6052e-45, 3.3561e-01, 5.6052e-45, 1.9345e-02, 5.6052e-45, 1.6015e-03,\n",
       "                      1.0143e-01, 2.6590e-02, 5.6052e-45, 3.2311e-07, 1.2057e-06, 8.1704e-03,\n",
       "                      2.1503e-03, 8.4116e-03, 5.6052e-45, 4.8275e-03, 2.0312e-01, 5.6052e-45,\n",
       "                      5.6052e-45, 2.9340e-03, 6.1994e-04, 5.6052e-45, 1.1087e-02, 2.6993e-04,\n",
       "                      5.7334e-03, 1.7615e-01, 7.6009e-02, 3.9053e-03])),\n",
       "             ('combine_layer.3.num_batches_tracked', tensor(1203)),\n",
       "             ('combine_layer.4.weight',\n",
       "              tensor([[ 0.0055, -0.1028,  0.0154,  ...,  0.0024, -0.0253,  0.0732],\n",
       "                      [ 0.0421,  0.0601, -0.0322,  ..., -0.0851,  0.0742, -0.0308],\n",
       "                      [ 0.0017,  0.0595,  0.0758,  ...,  0.0910, -0.0541, -0.0094],\n",
       "                      ...,\n",
       "                      [-0.0447,  0.0174,  0.1082,  ...,  0.0140,  0.0669,  0.0375],\n",
       "                      [-0.0438,  0.0572, -0.1022,  ...,  0.0786,  0.0012,  0.0362],\n",
       "                      [-0.0800,  0.0055,  0.0902,  ...,  0.0146,  0.0020, -0.0420]])),\n",
       "             ('combine_layer.4.bias',\n",
       "              tensor([ 0.0260,  0.0347,  0.0498,  0.0689, -0.0441,  0.0151,  0.0804, -0.0822,\n",
       "                       0.1399, -0.0645, -0.0356, -0.0143,  0.0108,  0.1014,  0.0489, -0.0729,\n",
       "                      -0.0375, -0.0432, -0.0518, -0.0110, -0.0194, -0.0742,  0.0182, -0.0344,\n",
       "                       0.0058,  0.0095,  0.1083,  0.0314,  0.0151, -0.0890,  0.1351, -0.0360,\n",
       "                      -0.0762, -0.1057,  0.1108, -0.0793,  0.0308, -0.0572, -0.0748,  0.0351,\n",
       "                      -0.0815,  0.0211, -0.0455,  0.0915, -0.0067, -0.0387, -0.1038, -0.0277,\n",
       "                       0.0325, -0.0875, -0.0202, -0.0335,  0.0767,  0.0119, -0.0507, -0.1098,\n",
       "                      -0.0892, -0.1020,  0.0500,  0.1204,  0.0731,  0.0499,  0.0299, -0.0116,\n",
       "                       0.0151, -0.0196,  0.1383, -0.0980, -0.0865,  0.0320,  0.0982, -0.0241,\n",
       "                      -0.0440, -0.0065, -0.0011,  0.0428, -0.0975,  0.0525, -0.0835,  0.0632,\n",
       "                      -0.0204,  0.0812, -0.0581, -0.0560,  0.1352,  0.0425, -0.0914,  0.0480,\n",
       "                       0.0614, -0.0019,  0.0587, -0.0011, -0.0482, -0.1411, -0.0458,  0.0533,\n",
       "                      -0.0390,  0.0396,  0.0103, -0.1139])),\n",
       "             ('combine_layer.7.weight',\n",
       "              tensor([[-0.0501, -0.0228,  0.1132,  ..., -0.0484, -0.0185,  0.0821],\n",
       "                      [ 0.0647, -0.0506,  0.0283,  ..., -0.0440,  0.0615, -0.0430],\n",
       "                      [ 0.0525,  0.0184, -0.0334,  ..., -0.0210,  0.0630, -0.0616],\n",
       "                      ...,\n",
       "                      [ 0.0898,  0.0890,  0.0707,  ..., -0.0345,  0.0405, -0.0619],\n",
       "                      [ 0.0087,  0.0704, -0.0188,  ...,  0.0438,  0.0446,  0.0847],\n",
       "                      [ 0.0791,  0.0311,  0.0963,  ...,  0.0168,  0.0684, -0.0117]])),\n",
       "             ('combine_layer.7.bias',\n",
       "              tensor([ 0.0148,  0.1106,  0.0493,  0.0193,  0.0504, -0.0184,  0.0239, -0.0097,\n",
       "                      -0.0040,  0.0338,  0.1055,  0.0194, -0.0048, -0.0749,  0.0290,  0.0603,\n",
       "                       0.1215, -0.0232,  0.1207,  0.1116,  0.1254,  0.0935,  0.0541, -0.0005,\n",
       "                      -0.0522, -0.0059,  0.0979,  0.0278,  0.0099,  0.1474,  0.0350,  0.0831,\n",
       "                       0.1019,  0.0768,  0.1009, -0.0308,  0.0241,  0.0282, -0.0572, -0.0320,\n",
       "                       0.0866,  0.0891, -0.0197,  0.0002,  0.1139, -0.0088,  0.0110,  0.0733,\n",
       "                       0.1359, -0.0292, -0.0114, -0.0107,  0.0067,  0.0319, -0.0062, -0.0563,\n",
       "                       0.0226, -0.0396,  0.0439, -0.0352, -0.0006,  0.0654, -0.0067,  0.0677,\n",
       "                       0.0599,  0.1196,  0.0169,  0.0544, -0.0182,  0.0583,  0.0657,  0.0988,\n",
       "                      -0.0138, -0.0095, -0.0473,  0.1096,  0.1023,  0.0660,  0.0536,  0.0275,\n",
       "                      -0.0072, -0.0146, -0.0202,  0.1702,  0.0526,  0.0467, -0.0139,  0.0715,\n",
       "                       0.1009,  0.0767,  0.0844,  0.0640, -0.0401,  0.1120, -0.0539,  0.0072,\n",
       "                       0.0112,  0.0493,  0.0132,  0.0813])),\n",
       "             ('combine_layer.9.weight',\n",
       "              tensor([0.9488, 0.9670, 0.9603, 0.9418, 0.9687, 0.9699, 0.9649, 0.9712, 0.9633,\n",
       "                      0.9709, 0.9544, 0.9628, 0.9722, 0.9790, 0.9751, 0.9717, 0.9566, 0.9656,\n",
       "                      0.9709, 0.9788, 0.9681, 0.9500, 0.9687, 0.9582, 0.9567, 0.9450, 0.9733,\n",
       "                      0.9695, 0.9771, 0.9840, 0.9778, 0.9696, 0.9642, 0.9830, 0.9622, 0.9616,\n",
       "                      0.9481, 0.9691, 0.9782, 0.9737, 0.9831, 0.9775, 0.9910, 0.9613, 0.9707,\n",
       "                      0.9735, 0.9576, 0.9797, 0.9599, 0.9714, 0.9620, 0.9865, 0.9749, 0.9537,\n",
       "                      0.9594, 0.9556, 0.9657, 0.9590, 0.9585, 0.9520, 0.9780, 0.9660, 0.9464,\n",
       "                      0.9842, 0.9802, 0.9645, 0.9673, 0.9788, 0.9524, 0.9466, 0.9638, 0.9387,\n",
       "                      0.9704, 0.9719, 0.9749, 0.9522, 0.9596, 0.9558, 0.9664, 0.9718, 0.9700,\n",
       "                      0.9624, 0.9607, 0.9393, 0.9559, 0.9878, 0.9841, 0.9609, 0.9736, 0.9649,\n",
       "                      0.9550, 0.9677, 0.9501, 0.9917, 0.9562, 0.9568, 0.9691, 0.9728, 0.9801,\n",
       "                      0.9663])),\n",
       "             ('combine_layer.9.bias',\n",
       "              tensor([ 0.0741, -0.0719, -0.0704,  0.0785, -0.0715, -0.0708, -0.0729,  0.0835,\n",
       "                      -0.0697, -0.0729,  0.0831,  0.0759,  0.0927, -0.0447,  0.0796,  0.0902,\n",
       "                      -0.0711, -0.0733, -0.0824, -0.0712, -0.0807, -0.0563, -0.0676,  0.0755,\n",
       "                      -0.0677, -0.0631, -0.0739, -0.0582,  0.0799,  0.0834, -0.0761, -0.0736,\n",
       "                      -0.0716,  0.0812, -0.0684,  0.0817,  0.1103,  0.0751, -0.0903, -0.0726,\n",
       "                       0.0860,  0.0851,  0.0773,  0.0733,  0.0755,  0.0937,  0.0995,  0.0932,\n",
       "                      -0.0700,  0.0777,  0.0748,  0.0830, -0.0673,  0.0891, -0.0606, -0.0707,\n",
       "                      -0.0717,  0.0843, -0.0816, -0.0657,  0.0829,  0.0982, -0.0536, -0.0703,\n",
       "                      -0.0751, -0.0737, -0.0700, -0.0738,  0.0848,  0.0694,  0.0715,  0.0701,\n",
       "                      -0.0736,  0.0831, -0.0736, -0.0915, -0.0699, -0.0716, -0.0682,  0.0765,\n",
       "                       0.0788, -0.0825, -0.0742,  0.0979, -0.0584,  0.0140,  0.0828,  0.0836,\n",
       "                      -0.0750, -0.0736,  0.0711,  0.0759, -0.0670,  0.0928, -0.0542, -0.0714,\n",
       "                      -0.0739, -0.0738, -0.0743, -0.0708])),\n",
       "             ('combine_layer.9.running_mean',\n",
       "              tensor([0.5062, 0.6719, 0.6598, 0.5600, 0.6138, 0.5529, 0.6408, 0.5680, 0.5423,\n",
       "                      0.6045, 0.6044, 0.5718, 0.6245, 0.1678, 0.6415, 0.4656, 0.6069, 0.7910,\n",
       "                      0.6116, 0.5719, 0.7072, 0.3541, 0.4733, 0.6262, 0.5108, 0.4364, 0.6602,\n",
       "                      0.1292, 0.6650, 0.6565, 0.6438, 0.7253, 0.6922, 0.6152, 0.4903, 0.5069,\n",
       "                      0.3911, 0.6030, 0.4423, 0.6315, 0.7285, 0.6975, 0.6914, 0.5540, 0.6766,\n",
       "                      0.3901, 0.5145, 0.6512, 0.6305, 0.4323, 0.6518, 0.5236, 0.3754, 0.5399,\n",
       "                      0.1141, 0.0833, 0.6460, 0.6495, 0.5725, 0.3247, 0.5449, 0.6492, 0.2246,\n",
       "                      0.5536, 0.7140, 0.6563, 0.5473, 0.6240, 0.5285, 0.4901, 0.5061, 0.4828,\n",
       "                      0.6212, 0.6100, 0.5893, 0.6376, 0.7019, 0.6567, 0.5265, 0.7182, 0.6318,\n",
       "                      0.5250, 0.4690, 0.2126, 0.1933, 0.1356, 0.5344, 0.6761, 0.6250, 0.5388,\n",
       "                      0.6662, 0.5157, 0.5657, 0.8621, 0.1220, 0.6860, 0.6746, 0.7256, 0.5347,\n",
       "                      0.5420])),\n",
       "             ('combine_layer.9.running_var',\n",
       "              tensor([0.5045, 0.4379, 0.5666, 0.4190, 0.4958, 0.5081, 0.5507, 0.3338, 0.4932,\n",
       "                      0.3709, 0.7902, 0.4659, 0.6206, 0.1114, 0.4575, 0.2979, 0.3941, 0.9152,\n",
       "                      0.4730, 0.2577, 0.4036, 0.1414, 0.4841, 0.4213, 0.5290, 0.2491, 0.4919,\n",
       "                      0.0467, 0.7074, 0.5316, 0.4848, 0.6910, 0.5333, 0.4117, 0.3795, 0.3184,\n",
       "                      0.2372, 0.4320, 0.4290, 0.7668, 0.8593, 0.5303, 0.6520, 0.6612, 0.5645,\n",
       "                      0.3703, 0.5105, 0.4855, 0.3942, 0.3873, 0.6997, 0.3548, 0.2602, 0.5031,\n",
       "                      0.0438, 0.0577, 0.6178, 0.4722, 0.4348, 0.3066, 1.0562, 0.7197, 0.1106,\n",
       "                      0.5452, 0.6077, 0.4256, 0.4349, 0.3997, 0.5194, 0.3438, 0.4945, 0.3644,\n",
       "                      0.5724, 0.6731, 0.6850, 0.5332, 0.4446, 0.4528, 0.5039, 0.6363, 0.6687,\n",
       "                      0.5633, 0.3630, 0.1328, 0.1139, 0.0765, 0.7543, 0.5573, 0.3686, 0.3753,\n",
       "                      0.5509, 0.3426, 0.4703, 1.1459, 0.0829, 0.7608, 0.7445, 0.6726, 0.4512,\n",
       "                      0.4282])),\n",
       "             ('combine_layer.9.num_batches_tracked', tensor(1203)),\n",
       "             ('combine_layer.10.weight',\n",
       "              tensor([[ 0.0279, -0.0666, -0.0442,  0.0634, -0.0305, -0.0278, -0.0814,  0.0457,\n",
       "                       -0.0423, -0.0776,  0.0322,  0.0694,  0.0325,  0.0035,  0.0877,  0.0366,\n",
       "                       -0.0197, -0.0744, -0.0233, -0.0198, -0.0479, -0.0035, -0.0196,  0.0726,\n",
       "                       -0.0482, -0.0279, -0.0811,  0.0003,  0.0918,  0.0626, -0.0659, -0.0797,\n",
       "                       -0.0825,  0.0842, -0.0212,  0.0271,  0.0165,  0.0824, -0.0185, -0.0640,\n",
       "                        0.0650,  0.0769,  0.0898,  0.0788,  0.0889,  0.0293,  0.0400,  0.0634,\n",
       "                       -0.0585,  0.0328,  0.0953,  0.0418, -0.0125,  0.0420,  0.0006, -0.0023,\n",
       "                       -0.0668,  0.0641, -0.0361, -0.0056,  0.0480,  0.0414,  0.0042, -0.0142,\n",
       "                       -0.0650, -0.0695, -0.0490, -0.0717,  0.0415,  0.0304,  0.0523,  0.0404,\n",
       "                       -0.0353,  0.0534, -0.0687, -0.0243, -0.0754, -0.0766, -0.0336,  0.0936,\n",
       "                        0.0984, -0.0263, -0.0315,  0.0148,  0.0021,  0.0059,  0.0390,  0.0616,\n",
       "                       -0.0894, -0.0286,  0.0750,  0.0568, -0.0415,  0.0519,  0.0038, -0.0432,\n",
       "                       -0.0726, -0.0850, -0.0189, -0.0593]])),\n",
       "             ('combine_layer.10.bias', tensor([0.0879]))])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the state_dict of the model\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Access the weights of the combine_layer\n",
    "model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_layer_weights = model_state_dict['phi_branch.0.weight'].zero_()\n",
    "\n",
    "\n",
    "\n",
    "# Load the modified state dict back into the model\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular NN (separate phi branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Fold 1:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0147\n",
      "  RMSE: 0.1213\n",
      "  MAE: 0.0765\n",
      "  R^2: 0.0779\n",
      "\n",
      "\n",
      "Metrics for Fold 2:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0119\n",
      "  RMSE: 0.1093\n",
      "  MAE: 0.0714\n",
      "  R^2: 0.0893\n",
      "\n",
      "\n",
      "Metrics for Fold 3:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0132\n",
      "  RMSE: 0.1149\n",
      "  MAE: 0.0724\n",
      "  R^2: 0.1036\n",
      "\n",
      "\n",
      "Metrics for Fold 4:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0113\n",
      "  RMSE: 0.1061\n",
      "  MAE: 0.0682\n",
      "  R^2: 0.1664\n",
      "\n",
      "\n",
      "Metrics for Fold 5:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0167\n",
      "  RMSE: 0.1293\n",
      "  MAE: 0.0818\n",
      "  R^2: 0.0707\n",
      "\n",
      "\n",
      "Metrics for Fold 6:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0110\n",
      "  RMSE: 0.1048\n",
      "  MAE: 0.0669\n",
      "  R^2: 0.1366\n",
      "\n",
      "\n",
      "Metrics for Fold 7:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0168\n",
      "  RMSE: 0.1297\n",
      "  MAE: 0.0781\n",
      "  R^2: 0.0675\n",
      "\n",
      "\n",
      "Metrics for Fold 8:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0183\n",
      "  RMSE: 0.1352\n",
      "  MAE: 0.0874\n",
      "  R^2: -0.0143\n",
      "\n",
      "\n",
      "Metrics for Fold 9:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0125\n",
      "  RMSE: 0.1116\n",
      "  MAE: 0.0742\n",
      "  R^2: 0.0175\n",
      "\n",
      "\n",
      "Metrics for Fold 10:\n",
      "Metrics for Test set:\n",
      "  MSE: 0.0181\n",
      "  RMSE: 0.1344\n",
      "  MAE: 0.0817\n",
      "  R^2: 0.0816\n",
      "\n",
      "\n",
      "Average Metrics Across Folds:\n",
      "  Average MSE: 0.0144\n",
      "  Average MAE: 0.0759\n",
      "  Average R2: 0.0797\n",
      "  STD MSE: 0.0027\n",
      "  STD MAE: 0.0062\n",
      "  STD R2: 0.0494\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_columns = ['X','Y','Z']\n",
    "# Function to print evaluation metrics\n",
    "def print_metrics(actual, predicted, set_name):\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "\n",
    "    print(f\"Metrics for {set_name} set:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R^2: {r2:.4f}\\n\")\n",
    "\n",
    "# Assuming deposit_data, covariates, and other necessary variables are defined\n",
    "\n",
    "# Create an array to store metrics for each fold\n",
    "test_mse_list = []\n",
    "test_mae_list = []\n",
    "test_r2_list = []\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 10\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Define a neural network with separate branches for phi_columns and covariates\n",
    "class GroupedNet(nn.Module):\n",
    "    def __init__(self, input_size_phi, input_size_covariates, output_size):\n",
    "        super(GroupedNet, self).__init__()\n",
    "        self.phi_branch = nn.Sequential(\n",
    "            nn.Linear(in_features=input_size_phi, out_features=1)\n",
    "     \n",
    "        )\n",
    "\n",
    "        self.combine_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=input_size_covariates+1, out_features=100), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Linear(in_features=100, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=100, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Linear(in_features=100, out_features=1)\n",
    "        )\n",
    "                                           \n",
    "\n",
    "    def forward(self, input_phi, input_covariates):\n",
    "        phi_output = self.phi_branch(input_phi)\n",
    "        x = torch.cat((phi_output, input_covariates), dim=1)\n",
    "        output = self.combine_layer(x)\n",
    "        return output\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(deposit_data)):\n",
    "    train_data, test_data = deposit_data.iloc[train_index], deposit_data.iloc[test_index]\n",
    "\n",
    "    x_train_phi = train_data[phi_columns].values\n",
    "    x_train_covariates = train_data[covariates].values\n",
    "    y_train = train_data['Density_gcm3'].values\n",
    "\n",
    "    x_test_phi = test_data[phi_columns].values\n",
    "    x_test_covariates = test_data[covariates].values\n",
    "    y_test = test_data['Density_gcm3'].values\n",
    "\n",
    "    # Define your neural network\n",
    "    model = GroupedNet(input_size_phi=len(phi_columns), input_size_covariates=len(covariates), output_size=1)\n",
    "\n",
    "    mse_loss = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_losses = []  # To store training losses during training\n",
    "    test_losses = []   # To store test losses during training\n",
    "\n",
    "    # Training loop\n",
    "    #for step in range(601):\n",
    "    for step in range(601):\n",
    "        pre = model(torch.tensor(x_train_phi, dtype=torch.float32), torch.tensor(x_train_covariates, dtype=torch.float32))\n",
    "        mse = mse_loss(pre, torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32))\n",
    "        cost = mse\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pre_test = model(torch.tensor(x_test_phi, dtype=torch.float32), torch.tensor(x_test_covariates, dtype=torch.float32))\n",
    "        mse_test = mse_loss(pre_test, torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32))\n",
    "        test_losses.append(mse_test.item())\n",
    "\n",
    "    # Store metrics for this fold\n",
    "    test_predictions_fold = model(torch.tensor(x_test_phi, dtype=torch.float32), torch.tensor(x_test_covariates, dtype=torch.float32)).detach().numpy().flatten()\n",
    "    test_mse_list.append(mean_squared_error(y_test, test_predictions_fold))\n",
    "    test_mae_list.append(mean_absolute_error(y_test, test_predictions_fold))\n",
    "    test_r2_list.append(r2_score(y_test, test_predictions_fold))\n",
    "\n",
    "    # Print metrics for the current fold\n",
    "    print(f\"\\nMetrics for Fold {fold + 1}:\")\n",
    "    print_metrics(y_test, test_predictions_fold, \"Test\")\n",
    "\n",
    "# Print average\n",
    "# Print average metrics across folds\n",
    "print(\"\\nAverage Metrics Across Folds:\")\n",
    "print(f\"  Average MSE: {np.mean(test_mse_list):.4f}\")\n",
    "print(f\"  Average MAE: {np.mean(test_mae_list):.4f}\")\n",
    "print(f\"  Average R2: {np.mean(test_r2_list):.4f}\")\n",
    "print(f\"  STD MSE: {np.std(test_mse_list):.4f}\")\n",
    "print(f\"  STD MAE: {np.std(test_mae_list):.4f}\")\n",
    "print(f\"  STD R2: {np.std(test_r2_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepkriging train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Test set:\n",
      "  MSE: 0.0030\n",
      "  RMSE: 0.0546\n",
      "  MAE: 0.0379\n",
      "  R^2: 0.7951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Function to print evaluation metrics\n",
    "def print_metrics(actual, predicted, set_name):\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "\n",
    "    print(f\"Metrics for {set_name} set:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R^2: {r2:.4f}\\n\")\n",
    "\n",
    "# Assuming deposit_data, covariates, and other necessary variables are defined\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(deposit_data[phi_columns + covariates],\n",
    "                                                    deposit_data['Density_gcm3'],\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your neural network\n",
    "class GroupedNet(nn.Module):\n",
    "    def __init__(self, input_size_phi, input_size_covariates, output_size):\n",
    "        super(GroupedNet, self).__init__()\n",
    "        self.phi_branch = nn.Sequential(\n",
    "            nn.Linear(in_features=input_size_phi, out_features=1)\n",
    "        )\n",
    "\n",
    "        self.combine_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=input_size_covariates+1, out_features=100), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Linear(in_features=100, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=100, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Linear(in_features=100, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_phi, input_covariates):\n",
    "        phi_output = self.phi_branch(input_phi)\n",
    "        x = torch.cat((phi_output, input_covariates), dim=1)\n",
    "        output = self.combine_layer(x)\n",
    "        return output\n",
    "\n",
    "model = GroupedNet(input_size_phi=len(phi_columns), input_size_covariates=len(covariates), output_size=1)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []  # To store training losses during training\n",
    "test_losses = []   # To store test losses during training\n",
    "\n",
    "# Training loop\n",
    "for step in range(601):\n",
    "    pre = model(torch.tensor(X_train[phi_columns].values, dtype=torch.float32),\n",
    "                torch.tensor(X_train[covariates].values, dtype=torch.float32))\n",
    "    \n",
    "    mse = mse_loss(pre, torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32))\n",
    "    cost = mse\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "# Print metrics for the test set\n",
    "test_predictions = model(torch.tensor(X_test[phi_columns].values, dtype=torch.float32),\n",
    "                         torch.tensor(X_test[covariates].values, dtype=torch.float32)).detach().numpy().flatten()\n",
    "\n",
    "print_metrics(y_test.values, test_predictions, \"Test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geostatistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
