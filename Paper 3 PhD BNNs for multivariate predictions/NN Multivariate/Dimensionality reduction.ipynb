{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess your data\n",
    "data = pd.read_csv('../Curated_data/two_composite_filtered.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Column filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Filter useless columns\n",
    "\n",
    "First, filter for the columns that have severe problems, like too many NA's, or all same values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Diameter: all NaNs <br>\n",
    "Cumulate: all NaNs <br>\n",
    "Dataset: all 0s  <br>\n",
    "Recovery_m: all Nans <br>\n",
    "Recovery_Pct: all Nans <br>\n",
    "Re_ppm: all Nans <br>\n",
    "\n",
    "Col numbers: 17, 19, 20, 86, 87, 143\n",
    "Core Diameter, Cumulate, Dataset, Recovery_m, Recovery_pct, Re_ppm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to exclude\n",
    "columns_to_exclude = [\"Core_Diameter\", \"Cumulate\", \"DataSet\", \"Recovery_m\", \"Recovery_Pct\", \"Re_ppm\"]\n",
    "\n",
    "# Drop the specified columns from the dataset\n",
    "data = data.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Now, filtered_data contains all columns except the ones you wanted to exclude\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Filter uninteresting variables \n",
    "Second, get rid of the columns that are not physical variables of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Number <br>\n",
    "Length <br>\n",
    "CoreLoss_m <br>\n",
    "Date_Logged <br>\n",
    "Density <br>\n",
    "Density_kgm3 <br>\n",
    "Interval_Length <br>\n",
    "Logged_By <br>\n",
    "Ori_Confidence <br> \n",
    "samp_id <br>\n",
    "SampleID <br>\n",
    "\n",
    "Col numbers: 0, 7, 15, 16, 17, 19, 22, 27, 31, 65, 66  <br>\n",
    "Sample_Number, Length, Core_Loss_m, Date_Logged, Density, Density_kgm3, Interval_Length, Logged_by, Ori_Confidence, samp_id, SampleID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of additional columns to exclude\n",
    "additional_columns_to_exclude = [\"Sample Number\", \"Length\", \"CoreLoss_m\", \"Date_Logged\", \"Density\", \"Density_kgm3\", \"Interval_Length\", \"Logged_By\", \"Ori_Confidence\", \"samp_id\", \"SampleID\"]\n",
    "\n",
    "# Drop the specified columns from the dataset\n",
    "data = data.drop(columns=additional_columns_to_exclude)\n",
    "\n",
    "# Now, filtered_data contains all columns except the ones you wanted to exclude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter all the ones with the word \"Proportion\", as they are also useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter(regex='^(?!.*Proportion).*$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.3 Filter big proportion of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code deletes columns that have over 95% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of NaN values in each column\n",
    "nan_percentage = (data.isna().sum() / len(data)) * 100\n",
    "\n",
    "# Define a threshold (99% in this case)\n",
    "threshold = 90\n",
    "\n",
    "# Get the column indices that have less than the threshold percentage of NaN values\n",
    "columns_to_keep = nan_percentage[nan_percentage <= threshold].index\n",
    "\n",
    "# Create a new DataFrame with only the selected columns\n",
    "data = data[columns_to_keep]\n",
    "\n",
    "# Now, filtered_data contains only columns with less than 99% NaN values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "# Create a copy of the original DataFrame to store the encoded data\n",
    "encoded_data = data.copy()\n",
    "\n",
    "# Define the threshold for one-hot encoding (e.g., 10 unique values)\n",
    "threshold = 10\n",
    "\n",
    "# Iterate through each column\n",
    "for column in data.columns:\n",
    "    # Check if the column is of object data type (categorical)\n",
    "    if data[column].dtype == 'object':\n",
    "        unique_values = data[column].nunique()\n",
    "        \n",
    "        # Check if the number of unique values is within the threshold\n",
    "        if unique_values <= threshold:\n",
    "            # Perform one-hot encoding for columns with unique values within the threshold\n",
    "            encoded_columns = pd.get_dummies(encoded_data[column], prefix=column)\n",
    "            encoded_columns = encoded_columns.astype(int)  # Convert to integers (0 or 1)\n",
    "            encoded_data = pd.concat([encoded_data, encoded_columns], axis=1)\n",
    "            encoded_data = encoded_data.drop(columns=[column])\n",
    "\n",
    "# Now, 'encoded_data' contains the one-hot encoded columns within the specified threshold, with 0s and 1s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Filter for correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_numeric_columns(data):\n",
    "    # Select only columns with numeric data types (int or float)\n",
    "    numeric_columns = data.select_dtypes(include=['number'])\n",
    "    \n",
    "    return numeric_columns\n",
    "\n",
    "\n",
    "\n",
    "# Filter non-numeric columns\n",
    "filtered_data = filter_numeric_columns(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = filtered_data.corr()\n",
    "\n",
    "# Create a heatmap without displaying numeric values inside the cells\n",
    "plt.figure(figsize=(24, 16))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=False)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Filtering variables with small correlation to variables of interest\n",
    "\n",
    "Our variables of interest are the elements + density (so no physical variables such as mag susc, etc) that have less than 80% missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Create a regex pattern to match variable names with \"pct,\" \"ppm,\" or \"ppb\"\n",
    "pattern = re.compile(r'.*(pct|ppm|ppb).*')\n",
    "\n",
    "# Create a list of variables that match the pattern\n",
    "variables_matching_pattern = [column for column in filtered_data.columns if pattern.search(column)]\n",
    "\n",
    "# Filter the data for the variables matching the pattern\n",
    "variables_of_interest = filtered_data[variables_matching_pattern]\n",
    "\n",
    "# Now, filtered_data contains only the variables that match the specified pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold for missing values (80% in this case)\n",
    "threshold = 80\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "missing_percentage = (variables_of_interest.isnull().sum() / len(variables_of_interest)) * 100\n",
    "\n",
    "# Get the column indices that have less than or equal to the threshold percentage of missing values\n",
    "columns_to_keep = missing_percentage[missing_percentage <= threshold].index\n",
    "\n",
    "# Create a new DataFrame with only the selected columns\n",
    "variables_of_interest = variables_of_interest[columns_to_keep]\n",
    "\n",
    "# Now, filtered_data contains only the variables with less than or equal to 80% missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_of_interest.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = filtered_data.corr()\n",
    "\n",
    "# Define the threshold correlation value (absolute value)\n",
    "threshold = 0.2\n",
    "\n",
    "# Initialize the list of variables of interest with \"Density_gcm3\"\n",
    "variables_of_interest = ['Au_ppb', 'Pd_ppb', 'Pt_ppb', 'Co_ppm', 'Cr_ppm', 'Cu_pct', 'Fe_pct',\n",
    "       'Mn_ppm', 'Ni_pct', 'Pb_ppm', 'S_pct', 'SulphTot_pct', 'Zn_ppm', 'Density_gcm3']\n",
    "\n",
    "# Create a list to store the variables that should be kept\n",
    "variables_to_keep = [\"X\", \"Y\", \"Z\"]\n",
    "\n",
    "# Create a list to store the variables to remove\n",
    "variables_to_remove = []\n",
    "\n",
    "# Iterate through the columns and check the correlation with variables of interest\n",
    "for column in corr_matrix.columns:\n",
    "    if column in variables_of_interest:\n",
    "        other_variables = [var for var in variables_of_interest if var != column]\n",
    "        # Check if the minimum absolute correlation with the other variables of interest is smaller than the threshold\n",
    "        if abs(corr_matrix[column][other_variables]).max() < threshold:\n",
    "            variables_to_remove.append(column)\n",
    "    else:\n",
    "        # Check if the minimum absolute correlation with the variables of interest is smaller than the threshold\n",
    "        if abs(corr_matrix[column][variables_of_interest]).max() < threshold:\n",
    "            variables_to_remove.append(column)\n",
    "\n",
    "\n",
    "variables_to_remove.append(\"Alt1_Int_tr\")\n",
    "\n",
    "# Drop the variables with correlation (absolute value) smaller than the threshold\n",
    "remaining_data = filtered_data.drop(columns=variables_to_remove)\n",
    "\n",
    "# Add the variables to keep back to the remaining_data\n",
    "remaining_data[variables_to_keep] = filtered_data[variables_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = remaining_data.corr()\n",
    "\n",
    "# Create a heatmap without displaying numeric values inside the cells\n",
    "plt.figure(figsize=(18, 12))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=False)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = remaining_data.drop(columns=['Density_gcm3'])\n",
    "y = data['Density_gcm3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values in X with zeros\n",
    "X_filled = X.fillna(0)\n",
    "\n",
    "# Now X_filled contains missing values replaced with 0s\n",
    "X_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize the data in X_filled\n",
    "X_normalized = scaler.fit_transform(X_filled)\n",
    "\n",
    "# Now X_normalized contains the normalized data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Preprocess your data, including handling missing values and normalization\n",
    "\n",
    "# Define the architecture of the autoencoder\n",
    "input_dim = X_normalized.shape[1]  # Number of input features\n",
    "encoding_dim = 10  # You can adjust this for dimensionality reduction\n",
    "\n",
    "autoencoder = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(input_dim,)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(encoding_dim, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(input_dim)\n",
    "])\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X, X, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Extract feature importance from the bottleneck layer\n",
    "encoder_layer = autoencoder.layers[3]  # Choose the bottleneck layer\n",
    "encoded_X = encoder_layer.predict(X)\n",
    "\n",
    "# You can analyze the encoded_X to identify important features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
